{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "DL_MODEL_PC",
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bTj0RTm7IhO",
    "colab_type": "text"
   },
   "source": [
    "# Aspect Based Sentiment Analysis\n",
    "\n",
    "- This Notebook Contains out Bi-LSTM Based Sentiment Classification Model + Other LSTM + Attention Based Models we experimented with.\n",
    "\n",
    "#### References:\n",
    "- https://github.com/mjain72/Sentiment-Analysis-using-Word2Vec-and-LSTM/blob/master/SentimentAnalysisTwitter.py\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention\n",
    "- https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "- https://github.com/rohit-gupta/POS-Attention/blob/master/pos-attention-model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUJk8M4DZl5-",
    "colab_type": "text"
   },
   "source": [
    "\"\"\" AIT726 Final project - Part 2 - Polarity Classification using Deep learning models on SemEvalâ€™16 dataset ( 1708 training dataset and 587 testing dataset ) and Foursquare ( 849 testing dataset )\n",
    "\n",
    "Authors: Yasas, Prashanti, Ashwini\n",
    "\n",
    "Command to run the file: run DL_MODEL_PC.ipynb\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Xo1IhwIUbhEU",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/content/project')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oHYqgdpybrbS",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "%matplotlib inline"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7lHx3vu8Xep",
    "colab_type": "text"
   },
   "source": [
    "## Setup Requirements\n",
    "\n",
    "- Load Libraries and Required Resources"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6ABv6RVHcGK3",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import cross_validate, KFold, train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.metrics import confusion_matrix, roc_curve,  roc_auc_score, classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Attention, Bidirectional\n",
    "from tensorflow.keras.layers import Lambda, dot, Activation, concatenate\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# Load From Project\n",
    "from absa.config import DATA_PATHS\n",
    "from absa.dataset import load_dataset"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iuCQzLGT8f6-",
    "colab_type": "code",
    "outputId": "db321bb3-a414-41b0-9705-a27d1631005c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    }
   },
   "source": [
    "#Pass sentences through spacy nlp pipeline and get the output terms\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "#Create a set of stopwords\n",
    "stopwords = set(stopwords.words('english'))"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UERmy6HlaOrC",
    "colab_type": "text"
   },
   "source": [
    "# \n",
    "create word2vec (  pre trained word embeddings ) from \n",
    "GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qsCQqdHN8hRE",
    "colab_type": "code",
    "outputId": "269cdede-d8bd-4d43-f3bf-403562e8748d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    }
   },
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format('/content/resources/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "'Word2Vec Vector Size: %d' % word2vec.vector_size"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Word2Vec Vector Size: 300'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iP9IXRz9aWDe",
    "colab_type": "text"
   },
   "source": [
    "    \"\"\"Load and display semeval16 training dataset\n",
    "       Load - Load dataset using load_dataset method ( Reads formatted XML file from the provided path )\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fz_g_DATcHbp",
    "colab_type": "code",
    "outputId": "06e68a35-70df-496e-f00a-18ba96abbda9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    }
   },
   "source": [
    "train_ds_path = DATA_PATHS['asba.semeval16.raw.train']\n",
    "\n",
    "df_train = load_dataset(train_ds_path)\n",
    "\n",
    "df_train = df_train.loc[:, ['id', 'text', 'category', 'polarity']]\n",
    "\n",
    "df_train = pd.DataFrame({\n",
    "    'polarity': df_train.groupby(['id', 'text', 'category'])['polarity'].apply(list),\n",
    "}).reset_index()\n",
    "\n",
    "df_train.head()"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1004293:0</td>\n",
       "      <td>Judging from previous posts this used to be a ...</td>\n",
       "      <td>RESTAURANT#GENERAL</td>\n",
       "      <td>[negative]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1004293:1</td>\n",
       "      <td>We, there were four of us, arrived at noon - t...</td>\n",
       "      <td>SERVICE#GENERAL</td>\n",
       "      <td>[negative]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1004293:2</td>\n",
       "      <td>They never brought us complimentary noodles, i...</td>\n",
       "      <td>SERVICE#GENERAL</td>\n",
       "      <td>[negative]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004293:3</td>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>FOOD#QUALITY</td>\n",
       "      <td>[negative]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004293:3</td>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>FOOD#STYLE_OPTIONS</td>\n",
       "      <td>[negative]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  ...    polarity\n",
       "0  1004293:0  ...  [negative]\n",
       "1  1004293:1  ...  [negative]\n",
       "2  1004293:2  ...  [negative]\n",
       "3  1004293:3  ...  [negative]\n",
       "4  1004293:3  ...  [negative]\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 8
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFx2TZ6vakus",
    "colab_type": "text"
   },
   "source": [
    "    \"\"\"Load and display semeval16 testing dataset\n",
    "       Load - Load dataset using load_dataset method ( Reads formatted XML file from the provided path )\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tL4A5gO38oiT",
    "colab_type": "code",
    "outputId": "40eeb61d-76df-4485-a22b-4b381bcf4211",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    }
   },
   "source": [
    "test_ds_path = DATA_PATHS['asba.semeval16.raw.test.gold']\n",
    "\n",
    "df_test = load_dataset(test_ds_path)\n",
    "\n",
    "df_test = df_test.loc[:, ['id', 'text', 'category', 'polarity']]\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    'polarity': df_test.groupby(['id', 'text', 'category'])['polarity'].apply(list),\n",
    "}).reset_index()\n",
    "\n",
    "df_test.head()"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en_BlueRibbonSushi_478218171:0</td>\n",
       "      <td>Yum!</td>\n",
       "      <td>FOOD#QUALITY</td>\n",
       "      <td>[positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en_BlueRibbonSushi_478218171:1</td>\n",
       "      <td>Serves really good sushi.</td>\n",
       "      <td>FOOD#QUALITY</td>\n",
       "      <td>[positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en_BlueRibbonSushi_478218171:2</td>\n",
       "      <td>Not the biggest portions but adequate.</td>\n",
       "      <td>FOOD#STYLE_OPTIONS</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en_BlueRibbonSushi_478218171:3</td>\n",
       "      <td>Green Tea creme brulee is a must!</td>\n",
       "      <td>FOOD#QUALITY</td>\n",
       "      <td>[positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en_BlueRibbonSushi_478218171:4</td>\n",
       "      <td>Don't leave the restaurant without it.</td>\n",
       "      <td>FOOD#QUALITY</td>\n",
       "      <td>[positive]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id  ...    polarity\n",
       "0  en_BlueRibbonSushi_478218171:0  ...  [positive]\n",
       "1  en_BlueRibbonSushi_478218171:1  ...  [positive]\n",
       "2  en_BlueRibbonSushi_478218171:2  ...   [neutral]\n",
       "3  en_BlueRibbonSushi_478218171:3  ...  [positive]\n",
       "4  en_BlueRibbonSushi_478218171:4  ...  [positive]\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 10
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQz_ULBFaolg",
    "colab_type": "text"
   },
   "source": [
    "    \"\"\"Load and display Foursquare testing dataset\n",
    "       Load - Load dataset using load_dataset method ( Reads formatted XML file from the provided path )\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BJy_9SqD8y8I",
    "colab_type": "code",
    "outputId": "a5b41e39-2ae5-4d2c-b772-e92c4e970056",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    }
   },
   "source": [
    "test_fs_ds_path = DATA_PATHS['asba.foursquare.raw.test.gold']\n",
    "\n",
    "df_test_fs = load_dataset(test_fs_ds_path)\n",
    "\n",
    "df_test_fs = df_test_fs.loc[:, ['id', 'text', 'category', 'polarity']]\n",
    "\n",
    "df_test_fs = pd.DataFrame({\n",
    "    'polarity': df_test_fs.groupby(['id', 'text', 'category'])['polarity'].apply(list),\n",
    "}).reset_index()\n",
    "\n",
    "df_test_fs.head()"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0:0</td>\n",
       "      <td>2 words -filter coffee :-)</td>\n",
       "      <td>DRINKS#QUALITY</td>\n",
       "      <td>[negative]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101:0</td>\n",
       "      <td>Absolute favourite hotpot in town!</td>\n",
       "      <td>RESTAURANT#GENERAL</td>\n",
       "      <td>[positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102:0</td>\n",
       "      <td>Great great food, not too pricey, good service...</td>\n",
       "      <td>FOOD#QUALITY</td>\n",
       "      <td>[positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102:0</td>\n",
       "      <td>Great great food, not too pricey, good service...</td>\n",
       "      <td>LOCATION#GENERAL</td>\n",
       "      <td>[positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102:0</td>\n",
       "      <td>Great great food, not too pricey, good service...</td>\n",
       "      <td>RESTAURANT#PRICES</td>\n",
       "      <td>[positive]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  ...    polarity\n",
       "0    0:0  ...  [negative]\n",
       "1  101:0  ...  [positive]\n",
       "2  102:0  ...  [positive]\n",
       "3  102:0  ...  [positive]\n",
       "4  102:0  ...  [positive]\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 11
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xD3_t-qCavS8",
    "colab_type": "text"
   },
   "source": [
    "\"\"\"As we have multiple aspects we have used MultiLabelBinarizer to create y_train, y_test and y_test_fs\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "et5IBOEQcOg0",
    "colab_type": "code",
    "outputId": "e4b6be40-e102-4b76-9b73-40c669e7353a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "y_train = mlb.fit_transform(df_train.polarity)\n",
    "\n",
    "y_test = mlb.transform(df_test.polarity)\n",
    "\n",
    "y_test_fs = mlb.transform(df_test_fs.polarity)\n",
    "\n",
    "y_train.shape, y_test.shape, y_test_fs.shape"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((2258, 3), (743, 3), (1034, 3))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 12
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06LeY2Tfa1fd",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "\"\"\" Preprocess features - Train and test data , pos are passed through spacy_doc pipeline to generate the list of tokens, which are later passed through keras tokenizer to convert them to integers. Finally they are padded to maxlenght . By performing above steps we have our x_train, x_test and x_test_fs input features \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LyEbXawNeSHG",
    "colab_type": "code",
    "outputId": "b2213258-8b10-4294-a38f-5a80d5222681",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "maxlen = 50\n",
    "\n",
    "train_tokens = []\n",
    "train_pos_tags = []\n",
    "\n",
    "for sent in df_train.text:\n",
    "  spacy_doc = nlp(sent)\n",
    "  train_tokens.append([t.text for t in spacy_doc])\n",
    "  train_pos_tags.append([t.pos_ for t in spacy_doc])\n",
    "\n",
    "test_tokens = []\n",
    "test_pos_tags = []\n",
    "\n",
    "for sent in df_test.text:\n",
    "  spacy_doc = nlp(sent)\n",
    "  test_tokens.append([t.text for t in spacy_doc])\n",
    "  test_pos_tags.append([t.pos_ for t in spacy_doc])\n",
    "\n",
    "test_fs_tokens = []\n",
    "test_fs_pos_tags = []\n",
    "\n",
    "for sent in df_test_fs.text:\n",
    "  spacy_doc = nlp(sent)\n",
    "  test_fs_tokens.append([t.text for t in spacy_doc])\n",
    "  test_fs_pos_tags.append([t.pos_ for t in spacy_doc])\n",
    "\n",
    "#Convert words to integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_tokens + test_tokens + test_fs_tokens)\n",
    "x_train = tokenizer.texts_to_sequences(train_tokens)\n",
    "x_test = tokenizer.texts_to_sequences(test_tokens)\n",
    "x_test_fs = tokenizer.texts_to_sequences(test_fs_tokens)\n",
    "\n",
    "#Convert POS to integers\n",
    "pos_tokenizer = Tokenizer()\n",
    "pos_tokenizer.fit_on_texts(train_pos_tags + test_pos_tags + test_fs_pos_tags)\n",
    "x_pos_train = pos_tokenizer.texts_to_sequences(train_pos_tags)\n",
    "x_pos_test = pos_tokenizer.texts_to_sequences(test_pos_tags)\n",
    "x_pos_test_fs = pos_tokenizer.texts_to_sequences(test_fs_pos_tags)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_pos_train = pad_sequences(x_pos_train, maxlen=maxlen)\n",
    "\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "x_pos_test = pad_sequences(x_pos_test, maxlen=maxlen)\n",
    "\n",
    "x_test_fs = pad_sequences(x_test_fs, maxlen=maxlen)\n",
    "x_pos_test_fs = pad_sequences(x_pos_test_fs, maxlen=maxlen)\n",
    "\n",
    "x_train.shape, x_test.shape, x_test_fs.shape"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((2258, 50), (743, 50), (1034, 50))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 13
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-eksef7vb7KF",
    "colab_type": "text"
   },
   "source": [
    "create aspect_encoder using LabelBinarizer() to be used later in the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AVyZ5Vfnbfd6",
    "colab_type": "code",
    "outputId": "98ffe330-4039-4379-f280-8f01c2e12c51",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "aspect_encoder = LabelBinarizer()\n",
    "\n",
    "x_train_aspect = aspect_encoder.fit_transform(df_train.category)\n",
    "x_test_aspect = aspect_encoder.transform(df_test.category)\n",
    "x_test_fs_aspect = aspect_encoder.transform(df_test_fs.category)\n",
    "\n",
    "x_train_aspect.shape, x_test_aspect.shape, x_test_fs_aspect.shape"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((2258, 12), (743, 12), (1034, 12))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 14
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EO5wTvf09tUw",
    "colab_type": "text"
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtlzWJ91ct28",
    "colab_type": "text"
   },
   "source": [
    "\"\"\" create word embeddings and embedding_index based on word2vec \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C1GMwn81ly-F",
    "colab_type": "code",
    "outputId": "9f6e5368-13d7-4790-c2f5-74c4913af738",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    }
   },
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "seq_length = maxlen\n",
    "embedding_dim = word2vec.syn0.shape[1]\n",
    "vocab_size = len(word_index) + 1\n",
    "\n",
    "embeddings_index = word2vec\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "  if word in embeddings_index:\n",
    "    embedding_vector = embeddings_index[word]\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "  else:\n",
    "      pass # for words not in embedding index"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  after removing the cwd from sys.path.\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "w3EJJM-HDLMR",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#Extract vocabulary size and word embedding dimension\n",
    "max_tokens, dimension = len(pos_tokenizer.word_index)+1, 100"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GO5NUeH4dHLl",
    "colab_type": "text"
   },
   "source": [
    "\"\"\" Metrics on_train_begin and on_epoch_end are callback functions used to print intermediate results during training\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NPmZz_6-ZD8Z",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class Metrics(Callback):\n",
    "  def __init__(self, model, validation_data):\n",
    "    self.model_ = model\n",
    "    self.validation_data = validation_data\n",
    "\n",
    "  def on_train_begin(self, logs={}):\n",
    "    print('epoch, precision_micro, recall_micro, f1_micro')\n",
    "  \n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    val_predict = (np.asarray(self.model_.predict(self.validation_data[0]))).round()\n",
    "    val_targ = self.validation_data[1]\n",
    "    _val_precision = precision_score(val_targ, val_predict, average='micro')\n",
    "    _val_recall = recall_score(val_targ, val_predict, average='micro')\n",
    "    _val_f1 = f1_score(val_targ, val_predict, average='micro')\n",
    "    print('%d, %f, %f, %f' % (epoch + 1, _val_precision, _val_recall, _val_f1))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3qWslFldUAK",
    "colab_type": "text"
   },
   "source": [
    "\"\"\" Attention layer is created , reference to the source code usage has been provided \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UgknNmzyR2O1",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# from https://github.com/philipperemy/keras-attention-mechanism/blob/master/attention/attention.py\n",
    "\n",
    "def attention_3d_block(hidden_states, output_size=32):\n",
    "    \"\"\"\n",
    "    Many-to-one attention mechanism for Keras.\n",
    "    @param hidden_states: 3D tensor with shape (batch_size, time_steps, input_dim).\n",
    "    @return: 2D tensor with shape (batch_size, 64)\n",
    "    @author: felixhao28.\n",
    "    \"\"\"\n",
    "    hidden_size = int(hidden_states.shape[2])\n",
    "    # Inside dense layer\n",
    "    #              hidden_states            dot               W            =>           score_first_part\n",
    "    # (batch_size, time_steps, hidden_size) dot (hidden_size, hidden_size) => (batch_size, time_steps, hidden_size)\n",
    "    # W is the trainable weight matrix of attention Luong's multiplicative style score\n",
    "    score_first_part = Dense(hidden_size, use_bias=False, name='attention_score_vec')(hidden_states)\n",
    "    #            score_first_part           dot        last_hidden_state     => attention_weights\n",
    "    # (batch_size, time_steps, hidden_size) dot   (batch_size, hidden_size)  => (batch_size, time_steps)\n",
    "    h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name='last_hidden_state')(hidden_states)\n",
    "    score = dot([score_first_part, h_t], [2, 1], name='attention_score')\n",
    "    attention_weights = Activation('softmax', name='attention_weight')(score)\n",
    "    # (batch_size, time_steps, hidden_size) dot (batch_size, time_steps) => (batch_size, hidden_size)\n",
    "    context_vector = dot([hidden_states, attention_weights], [1, 1], name='context_vector')\n",
    "    pre_activation = concatenate([context_vector, h_t], name='attention_output')\n",
    "    attention_vector = Dense(output_size, use_bias=False, activation='tanh', name='attention_vector')(pre_activation)\n",
    "    return attention_vector"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSHv1hljdZEM",
    "colab_type": "text"
   },
   "source": [
    "\"\"\"create_model - Create different models such as 'LSTM', 'self-attention', 'pos-attention', 'bidirectional. For all the models we have one layer of 64 hidden units, and a fully connected output layer using sigmoid as activation function. We have used Adam optimizer, and cross-entropy for the loss function with learning rate 0.001 for all the models.\n",
    "In addition to aspect classification, here in polarity classification we are passing aspect dimention and aspect encoder to the concatenation layer\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "URTgKkfOfAKw",
    "colab_type": "code",
    "outputId": "bdd789dd-0931-4c64-d374-a876644ea621",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    }
   },
   "source": [
    "aspects_dim = 12\n",
    "lstm_dropout = 0.8\n",
    "lstm_out = 64\n",
    "use_model =  ['LSTM', 'self-attention', 'pos-attention', 'bidirectional'][3]\n",
    "loss = 'binary_crossentropy'\n",
    "def create_model():\n",
    "  optimizer = optimizers.Adam(learning_rate=0.0001)\n",
    "  embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=seq_length)\n",
    "\n",
    "  # Word Embedding\n",
    "  word_input_layer = Input(shape=(seq_length,))\n",
    "  word_embedding_output = embedding_layer(word_input_layer)\n",
    "\n",
    "  # POS Embedding\n",
    "  if use_model == 'pos-attention':\n",
    "    pos_embedding_layer = Embedding(max_tokens, dimension, input_length=seq_length)\n",
    "    pos_input_layer = Input(shape=(seq_length,), name='pos_seq')\n",
    "    pos_embedding_output = pos_embedding_layer(pos_input_layer)\n",
    "\n",
    "  # word_pos_attention_seq = Attention()([word_embedding_output, pos_embedding_output])\n",
    "\n",
    "  if use_model == 'self-attention':\n",
    "    lstm_output = LSTM(units=lstm_out, dropout=lstm_dropout, return_sequences=True)(word_embedding_output)\n",
    "    lstm_output = attention_3d_block(lstm_output)\n",
    "  elif use_model == 'pos-attention':\n",
    "    pass\n",
    "  elif use_model == 'bidirectional':\n",
    "    lstm_output = Bidirectional(LSTM(units=lstm_out, dropout=lstm_dropout))(word_embedding_output)\n",
    "  else:\n",
    "    lstm_output = LSTM(units=lstm_out, dropout=lstm_dropout)(word_embedding_output)\n",
    "\n",
    "  aspect_input_layer = Input(shape=(aspects_dim, ))\n",
    "\n",
    "  features = concatenate([lstm_output, aspect_input_layer])\n",
    "  # features = lstm_output\n",
    "\n",
    "  output_layer = Dense(3, activation='sigmoid')(features)\n",
    "\n",
    "  model = Model(inputs=[word_input_layer, aspect_input_layer], outputs=output_layer)\n",
    "\n",
    "  model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "  return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.summary()"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 300)      1341900     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 128)          186880      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 12)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 140)          0           bidirectional[0][0]              \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3)            423         concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,529,203\n",
      "Trainable params: 1,529,203\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MecIuKJpeeDO",
    "colab_type": "text"
   },
   "source": [
    "## Cross Validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4on4BmlBd4UP",
    "colab_type": "text"
   },
   "source": [
    "\"\"\"Perform 5-fold cross validation on the training dataset and use Metrics to derive intermediate evaluation results and finally we are using these results we perform error analysis \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FCsNP8Dth4fB",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Five fold Cross-Validation \n",
    "if False: # Done\n",
    "  y_preds = np.zeros(y.shape)\n",
    "  kf = KFold(n_splits=5)\n",
    "  print(model.summary())\n",
    "  for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    model, metrics = None, None\n",
    "    print('Epoch: %d' % i)\n",
    "    X_aspect_train, X_aspect_test = X_aspect[train_index], X_aspect[test_index]\n",
    "    X_train, X_test, Y_train, Y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "    model = create_model()\n",
    "    metrics = Metrics(model, validation_data=((X_test, X_aspect_test), Y_test))\n",
    "    model.fit((X_train, X_aspect_train), Y_train, epochs=num_epochs, verbose=0, batch_size=batch_size, callbacks=[metrics])\n",
    "    # predict the results\n",
    "    score, acc = model.evaluate((X_test, X_aspect_test), Y_test, verbose=2, batch_size=batch_size)\n",
    "    y_pred = model.predict((X_test, X_aspect_test))\n",
    "    y_preds[test_index] = y_pred\n",
    "    y_pred_bool = y_pred > threshold\n",
    "    # ROC AUC curve\n",
    "    roc_auc = roc_auc_score(Y_test, y_pred)\n",
    "    df = evaluate(Y_test, y_pred_bool)\n",
    "    print(df.to_csv())\n",
    "    print(\"roc_auc, {}\".format(roc_auc))\n",
    "    df_train['predictions'] = [list(x) for x in mlb.inverse_transform(y_preds > 0.5)]\n",
    "    df_train.to_excel('.project/output/preds/pc_dl.xlsx')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McRV6LDye4f3",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "Model: \"model_3\"\n",
    "__________________________________________________________________________________________________\n",
    "Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    "input_7 (InputLayer)            [(None, 50)]         0                                            \n",
    "__________________________________________________________________________________________________\n",
    "embedding_3 (Embedding)         (None, 50, 300)      931500      input_7[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "bidirectional_3 (Bidirectional) (None, 128)          186880      embedding_3[0][0]                \n",
    "__________________________________________________________________________________________________\n",
    "input_8 (InputLayer)            [(None, 12)]         0                                            \n",
    "__________________________________________________________________________________________________\n",
    "concatenate_3 (Concatenate)     (None, 140)          0           bidirectional_3[0][0]            \n",
    "                                                                 input_8[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "dense_3 (Dense)                 (None, 3)            423         concatenate_3[0][0]              \n",
    "==================================================================================================\n",
    "Total params: 1,118,803\n",
    "Trainable params: 1,118,803\n",
    "Non-trainable params: 0\n",
    "__________________________________________________________________________________________________\n",
    "None\n",
    "Epoch: 0\n",
    "epoch, precision_micro, recall_micro, f1_micro\n",
    "1, 0.780269, 0.746781, 0.763158\n",
    "2, 0.747727, 0.706009, 0.726269\n",
    "3, 0.775281, 0.740343, 0.757409\n",
    "4, 0.776018, 0.736052, 0.755507\n",
    "5, 0.803167, 0.761803, 0.781938\n",
    "6, 0.818391, 0.763948, 0.790233\n",
    "7, 0.816964, 0.785408, 0.800875\n",
    "8, 0.814220, 0.761803, 0.787140\n",
    "9, 0.820046, 0.772532, 0.795580\n",
    "10, 0.842593, 0.781116, 0.810690\n",
    "11, 0.829885, 0.774678, 0.801332\n",
    "12, 0.834483, 0.778970, 0.805771\n",
    "13, 0.830275, 0.776824, 0.802661\n",
    "14, 0.841232, 0.761803, 0.799550\n",
    "15, 0.825287, 0.770386, 0.796892\n",
    "16, 0.827354, 0.791845, 0.809211\n",
    "17, 0.831081, 0.791845, 0.810989\n",
    "18, 0.816372, 0.791845, 0.803922\n",
    "19, 0.811816, 0.796137, 0.803900\n",
    "20, 0.815385, 0.796137, 0.805646\n",
    "15/15 - 0s - loss: 0.3440 - accuracy: 0.8031\n",
    "index,value\n",
    "accuracy,0.7522123893805309\n",
    "precision_macro,0.7444842994536663\n",
    "precision_micro,0.8153846153846154\n",
    "recall_macro,0.5672030651340996\n",
    "recall_micro,0.796137339055794\n",
    "f1_macro,0.5931287581186977\n",
    "f1_micro,0.8056460369163952\n",
    "\n",
    "roc_auc, 0.879261813802623\n",
    "Epoch: 1\n",
    "epoch, precision_micro, recall_micro, f1_micro\n",
    "1, 0.713592, 0.639130, 0.674312\n",
    "2, 0.793333, 0.776087, 0.784615\n",
    "3, 0.821918, 0.782609, 0.801782\n",
    "4, 0.819413, 0.789130, 0.803987\n",
    "5, 0.807606, 0.784783, 0.796031\n",
    "6, 0.837104, 0.804348, 0.820399\n",
    "7, 0.827586, 0.782609, 0.804469\n",
    "8, 0.847575, 0.797826, 0.821948\n",
    "9, 0.857477, 0.797826, 0.826577\n",
    "10, 0.869464, 0.810870, 0.839145\n",
    "11, 0.844749, 0.804348, 0.824053\n",
    "12, 0.851163, 0.795652, 0.822472\n",
    "13, 0.842105, 0.800000, 0.820513\n",
    "14, 0.848131, 0.789130, 0.817568\n",
    "15, 0.839161, 0.782609, 0.809899\n",
    "16, 0.831019, 0.780435, 0.804933\n",
    "17, 0.837104, 0.804348, 0.820399\n",
    "18, 0.819820, 0.791304, 0.805310\n",
    "19, 0.822616, 0.806522, 0.814490\n",
    "20, 0.817787, 0.819565, 0.818675\n",
    "15/15 - 0s - loss: 0.3358 - accuracy: 0.8053\n",
    "index,value\n",
    "accuracy,0.7676991150442478\n",
    "precision_macro,0.5921877736906639\n",
    "precision_micro,0.8177874186550976\n",
    "recall_macro,0.5438344159191669\n",
    "recall_micro,0.8195652173913044\n",
    "f1_macro,0.5518709320038776\n",
    "f1_micro,0.8186753528773072\n",
    "\n",
    "roc_auc, 0.8397482100147294\n",
    "Epoch: 2\n",
    "epoch, precision_micro, recall_micro, f1_micro\n",
    "1, 0.750000, 0.712418, 0.730726\n",
    "2, 0.802850, 0.736383, 0.768182\n",
    "3, 0.826698, 0.769063, 0.796840\n",
    "4, 0.841379, 0.797386, 0.818792\n",
    "5, 0.850117, 0.790850, 0.819413\n",
    "6, 0.822472, 0.797386, 0.809735\n",
    "7, 0.853547, 0.812636, 0.832589\n",
    "8, 0.833708, 0.808279, 0.820796\n",
    "9, 0.863962, 0.788671, 0.824601\n",
    "10, 0.846868, 0.795207, 0.820225\n",
    "11, 0.842956, 0.795207, 0.818386\n",
    "12, 0.837647, 0.775599, 0.805430\n",
    "13, 0.823144, 0.821351, 0.822246\n",
    "14, 0.823400, 0.812636, 0.817982\n",
    "15, 0.825000, 0.790850, 0.807564\n",
    "16, 0.828571, 0.821351, 0.824945\n",
    "17, 0.838269, 0.801743, 0.819599\n",
    "18, 0.812775, 0.803922, 0.808324\n",
    "19, 0.846330, 0.803922, 0.824581\n",
    "20, 0.832215, 0.810458, 0.821192\n",
    "15/15 - 0s - loss: 0.3165 - accuracy: 0.8097\n",
    "index,value\n",
    "accuracy,0.7920353982300885\n",
    "precision_macro,0.690357023690357\n",
    "precision_micro,0.8322147651006712\n",
    "recall_macro,0.576789501590668\n",
    "recall_micro,0.8104575163398693\n",
    "f1_macro,0.6096637973940336\n",
    "f1_micro,0.8211920529801325\n",
    "\n",
    "roc_auc, 0.858959524583201\n",
    "Epoch: 3\n",
    "epoch, precision_micro, recall_micro, f1_micro\n",
    "1, 0.618056, 0.588106, 0.602709\n",
    "2, 0.778291, 0.742291, 0.759865\n",
    "3, 0.773543, 0.759912, 0.766667\n",
    "4, 0.783599, 0.757709, 0.770437\n",
    "5, 0.806897, 0.773128, 0.789651\n",
    "6, 0.757720, 0.702643, 0.729143\n",
    "7, 0.815909, 0.790749, 0.803132\n",
    "8, 0.784922, 0.779736, 0.782320\n",
    "9, 0.762980, 0.744493, 0.753623\n",
    "10, 0.778742, 0.790749, 0.784699\n",
    "11, 0.789011, 0.790749, 0.789879\n",
    "12, 0.738046, 0.781938, 0.759358\n",
    "13, 0.780911, 0.792952, 0.786885\n",
    "14, 0.765823, 0.799559, 0.782328\n",
    "15, 0.779221, 0.792952, 0.786026\n",
    "16, 0.767033, 0.768722, 0.767877\n",
    "17, 0.777538, 0.792952, 0.785169\n",
    "18, 0.748963, 0.795154, 0.771368\n",
    "19, 0.752643, 0.784141, 0.768069\n",
    "20, 0.744283, 0.788546, 0.765775\n",
    "15/15 - 0s - loss: 0.5663 - accuracy: 0.7561\n",
    "index,value\n",
    "accuracy,0.7117516629711752\n",
    "precision_macro,0.6655625356938892\n",
    "precision_micro,0.7442827442827443\n",
    "recall_macro,0.5539475388821841\n",
    "recall_micro,0.788546255506608\n",
    "f1_macro,0.5457204991456531\n",
    "f1_micro,0.7657754010695188\n",
    "\n",
    "roc_auc, 0.8324862898171297\n",
    "Epoch: 4\n",
    "epoch, precision_micro, recall_micro, f1_micro\n",
    "1, 0.592018, 0.579176, 0.585526\n",
    "2, 0.724706, 0.668113, 0.695260\n",
    "3, 0.776744, 0.724512, 0.749719\n",
    "4, 0.710046, 0.674620, 0.691880\n",
    "5, 0.767654, 0.731020, 0.748889\n",
    "6, 0.767654, 0.731020, 0.748889\n",
    "7, 0.768879, 0.728850, 0.748330\n",
    "8, 0.775463, 0.726681, 0.750280\n",
    "9, 0.778037, 0.722343, 0.749156\n",
    "10, 0.798561, 0.722343, 0.758542\n",
    "11, 0.757075, 0.696312, 0.725424\n",
    "12, 0.770089, 0.748373, 0.759076\n",
    "13, 0.780822, 0.741866, 0.760845\n",
    "14, 0.762749, 0.746204, 0.754386\n",
    "15, 0.764302, 0.724512, 0.743875\n",
    "16, 0.768722, 0.757050, 0.762842\n",
    "17, 0.761161, 0.739696, 0.750275\n",
    "18, 0.763393, 0.741866, 0.752475\n",
    "19, 0.764192, 0.759219, 0.761697\n",
    "20, 0.750000, 0.767896, 0.758842\n",
    "15/15 - 0s - loss: 0.4530 - accuracy: 0.7517\n",
    "index,value\n",
    "accuracy,0.7028824833702882\n",
    "precision_macro,0.6144501367542726\n",
    "precision_micro,0.75\n",
    "recall_macro,0.5376422493670555\n",
    "recall_micro,0.7678958785249458\n",
    "f1_macro,0.532139823309548\n",
    "f1_micro,0.7588424437299035\n",
    "\n",
    "roc_auc, 0.8383861659676043\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kKTlvsNd8G9",
    "colab_type": "text"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Train the model on with validation data\n",
    "x_train_1, x_valid, x_train_aspect_1, x_valid_aspect, y_train_1, y_valid = train_test_split(x_train, x_train_aspect, y_train, test_size= 0.33, random_state = 24)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kSs0h88ph0zb",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#fit model\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "# metrics = Metrics(model, validation_data=((x_valid, x_valid_aspect), y_valid))\n",
    "\n",
    "model.fit((x_train_1, x_train_aspect_1), y_train_1, epochs=num_epochs, batch_size=batch_size, validation_data=((x_valid, x_valid_aspect), y_valid))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7BGK3SteQJX",
    "colab_type": "text"
   },
   "source": [
    "\"\"\" Fit model on the complete training data set \"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mGhdam1IJxwT",
    "colab_type": "code",
    "outputId": "a459de73-c81c-49cc-f942-d943bc33d687",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    }
   },
   "source": [
    "#fit model\n",
    "batch_size = 32\n",
    "num_epochs = 7\n",
    "\n",
    "model.fit((x_train, x_train_aspect), y_train, epochs=num_epochs, verbose=1, batch_size=batch_size)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "71/71 [==============================] - 1s 16ms/step - loss: 0.6435 - accuracy: 0.5642\n",
      "Epoch 2/7\n",
      "71/71 [==============================] - 1s 16ms/step - loss: 0.4917 - accuracy: 0.6466\n",
      "Epoch 3/7\n",
      "71/71 [==============================] - 1s 16ms/step - loss: 0.4707 - accuracy: 0.6479\n",
      "Epoch 4/7\n",
      "71/71 [==============================] - 1s 16ms/step - loss: 0.4593 - accuracy: 0.6585\n",
      "Epoch 5/7\n",
      "71/71 [==============================] - 1s 16ms/step - loss: 0.4338 - accuracy: 0.6864\n",
      "Epoch 6/7\n",
      "71/71 [==============================] - 1s 17ms/step - loss: 0.3880 - accuracy: 0.7378\n",
      "Epoch 7/7\n",
      "71/71 [==============================] - 1s 17ms/step - loss: 0.3702 - accuracy: 0.7595\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdea4278898>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 20
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLs7Yo-JeMZ0",
    "colab_type": "text"
   },
   "source": [
    "## Evaluate on the test data "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q9CkQ_Hgd3vR",
    "colab_type": "code",
    "outputId": "68af1034-64a7-4b1d-869a-827fa46e4a24",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    }
   },
   "source": [
    "print(model.summary())"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 300)      1341900     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 128)          186880      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 12)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 140)          0           bidirectional[0][0]              \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3)            423         concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,529,203\n",
      "Trainable params: 1,529,203\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDwlelxLePPD",
    "colab_type": "text"
   },
   "source": [
    "### SemEval-16 Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRf---nDemp-",
    "colab_type": "text"
   },
   "source": [
    "\"\"\" Model predict on SemEval-16 test dataset\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6URxaWZ0iUKO",
    "colab_type": "code",
    "outputId": "a8681140-bd69-471d-869b-ddace9de1a55",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    }
   },
   "source": [
    "threshold = 0.5\n",
    "\n",
    "# predict the results\n",
    "score, acc = model.evaluate((x_test, x_test_aspect), y_test, verbose=2, batch_size=batch_size)\n",
    "y_pred = model.predict((x_test, x_test_aspect))\n",
    "y_pred_bool = y_pred > threshold\n",
    "# ROC AUC curve\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "f1_micro = f1_score(y_test, y_pred_bool, average='micro')\n",
    "\n",
    "print(\"roc_auc, {}\".format(roc_auc))\n",
    "\n",
    "report = classification_report(y_test, y_pred_bool, target_names=list(mlb.classes_), output_dict=True)\n",
    "\n",
    "df = pd.DataFrame(report).transpose()\n",
    "\n",
    "print(df.to_csv())"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "24/24 - 0s - loss: 0.3399 - accuracy: 0.8116\n",
      "roc_auc, 0.8753558382365396\n",
      ",precision,recall,f1-score,support\n",
      "negative,0.6757990867579908,0.7589743589743589,0.714975845410628,195.0\n",
      "neutral,0.625,0.11904761904761904,0.19999999999999998,42.0\n",
      "positive,0.8714555765595463,0.896887159533074,0.8839884947267498,514.0\n",
      "micro avg,0.8121693121693122,0.8175765645805593,0.8148639681486397,751.0\n",
      "macro avg,0.7240848877725123,0.5916363791850173,0.5996547800457925,751.0\n",
      "weighted avg,0.8068694917036152,0.8175765645805593,0.8018513663710011,751.0\n",
      "samples avg,0.8088829071332436,0.819650067294751,0.8111260655002244,751.0\n",
      "\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WsJaI9rfw0l",
    "colab_type": "text"
   },
   "source": [
    "### Foursquare Review Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1M3doaHez6S",
    "colab_type": "text"
   },
   "source": [
    "\"\"\" Model predict on test Foursqure dataset\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lkSCZosoX1S_",
    "colab_type": "code",
    "outputId": "3fadff5a-cde5-4611-c574-68ae69736ffc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    }
   },
   "source": [
    "threshold = 0.5\n",
    "\n",
    "# predict the results\n",
    "y_pred_fs = model.predict((x_test_fs, x_test_fs_aspect))\n",
    "y_pred_fs_bool = y_pred_fs > threshold\n",
    "# ROC AUC curve\n",
    "# roc_auc = roc_auc_score(y_test_fs, y_pred_fs)\n",
    "# print(\"roc_auc, {}\".format(roc_auc))\n",
    "\n",
    "report = classification_report(y_test_fs, y_pred_fs_bool, target_names=list(mlb.classes_), output_dict=True)\n",
    "\n",
    "df = pd.DataFrame(report).transpose()\n",
    "\n",
    "print(df.to_csv())"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      ",precision,recall,f1-score,support\n",
      "negative,0.6229508196721312,0.6229508196721312,0.6229508196721312,183.0\n",
      "neutral,0.3333333333333333,0.16666666666666666,0.2222222222222222,18.0\n",
      "positive,0.9301204819277108,0.9146919431279621,0.9223416965352449,844.0\n",
      "micro avg,0.8698630136986302,0.8507177033492823,0.8601838413159167,1045.0\n",
      "macro avg,0.628801544977725,0.5681031431555866,0.5891715794765328,1045.0\n",
      "weighted avg,0.8660494610019024,0.8507177033492823,0.8578530065796619,1045.0\n",
      "samples avg,0.8515473887814313,0.8544487427466151,0.8507414571244358,1045.0\n",
      "\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqdzqH2yCtx4",
    "colab_type": "text"
   },
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7w5tHkbe7EM",
    "colab_type": "text"
   },
   "source": [
    "\"\"\" Save the required model, tokenizer and mlb for demo purposes \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4iqtO2X4gH02",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import pickle\n",
    "\n",
    "model.save('/content/project/output/models/dl_model_pc.h5')\n",
    "\n",
    "# saving tokenizer\n",
    "with open('/content/project/output/models/tokenizer_pc.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xUwgJ1DpGZc-",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# saving mlb\n",
    "with open('/content/project/output/models/mlb_pc.pickle', 'wb') as handle:\n",
    "    pickle.dump(mlb, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-90t3o-BGZyJ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# saving aspect encoder\n",
    "np.save('/content/project/output/models/aspect_enc_classes.npy', aspect_encoder.classes_)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4cEslv8VGdZG",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    ""
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}