{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "DL_MODEL_AC",
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HMtBWpbuLg7",
    "colab_type": "text"
   },
   "source": [
    "# Aspect Based Sentiment Analysis\n",
    "\n",
    "- This Notebook Contains out Bi-LSTM Based Aspect Classification Model + Other LSTM + Attention Based Models we experimented with.\n",
    "\n",
    "#### References:\n",
    "- https://github.com/mjain72/Sentiment-Analysis-using-Word2Vec-and-LSTM/blob/master/SentimentAnalysisTwitter.py\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention\n",
    "- https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "- https://github.com/rohit-gupta/POS-Attention/blob/master/pos-attention-model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVUe98VnWx0Z",
    "colab_type": "text"
   },
   "source": [
    "\"\"\"\n",
    "AIT726 Final project - Part 2 -  Aspect Classification using Deep learning models on SemEvalâ€™16 dataset ( 1708 training dataset and 587 testing dataset ) and Foursquare ( 849 testing dataset )\n",
    "\n",
    "Authors: Yasas, Prashanti, Ashwini\n",
    "\n",
    "Command to run the file: run DL_MODEL_AC.ipynb\n",
    "\n",
    "Note : For more details please check README file\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oHYqgdpybrbS",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Setup Notebook\n",
    "%matplotlib inline"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uI9Nfo09t9xJ",
    "colab_type": "text"
   },
   "source": [
    "## Setup Requirements\n",
    "\n",
    "- Load Libraries and Required Resources"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "76ae6CiC3gnX",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import cross_validate, KFold, train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.metrics import confusion_matrix, roc_curve,  roc_auc_score, classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Attention, Bidirectional\n",
    "from tensorflow.keras.layers import Lambda, dot, Activation, concatenate\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Load From Project\n",
    "from absa.config import DATA_PATHS\n",
    "from absa.dataset import load_dataset"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6ABv6RVHcGK3",
    "colab_type": "code",
    "outputId": "68903eb5-b412-4556-da19-d735946fb01a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    }
   },
   "source": [
    "import spacy\n",
    "#Pass sentences through spacy nlp pipeline and get the output terms\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "#Create a set of stopwords\n",
    "stopwords = set(stopwords.words('english'))"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1vB22cKG4bC",
    "colab_type": "text"
   },
   "source": [
    "# \n",
    "create word2vec (  pre trained word embeddings ) from \n",
    "GoogleNews-vectors-negative300.bin.gz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WNP9HMUIw6la",
    "colab_type": "code",
    "outputId": "6cf2d298-389b-47d2-d3bd-5232a50fcc32",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    }
   },
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format('/content/resources/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "'Word2Vec Vector Size: %d' % word2vec.vector_size"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Word2Vec Vector Size: 300'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 10
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZCTnwQkHnvq",
    "colab_type": "text"
   },
   "source": [
    "    \"\"\"Load and display semeval16 training dataset\n",
    "       Load - Load dataset using load_dataset method ( Reads formatted XML file from the provided path )\"\"\"\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fz_g_DATcHbp",
    "colab_type": "code",
    "outputId": "37fe5aa1-833e-4dac-ecc5-30774b523baa",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    }
   },
   "source": [
    "train_ds_path = DATA_PATHS['asba.semeval16.raw.train']\n",
    "\n",
    "df_train = load_dataset(train_ds_path)\n",
    "\n",
    "df_train = pd.DataFrame({\n",
    "    'text': df_train.groupby('id')['text'].first(),\n",
    "    'categories': df_train.groupby('id')['category'].apply(list),\n",
    "})\n",
    "\n",
    "df_train.head()"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1004293:0</th>\n",
       "      <td>Judging from previous posts this used to be a ...</td>\n",
       "      <td>[RESTAURANT#GENERAL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004293:1</th>\n",
       "      <td>We, there were four of us, arrived at noon - t...</td>\n",
       "      <td>[SERVICE#GENERAL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004293:2</th>\n",
       "      <td>They never brought us complimentary noodles, i...</td>\n",
       "      <td>[SERVICE#GENERAL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004293:3</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[FOOD#QUALITY, FOOD#STYLE_OPTIONS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004293:4</th>\n",
       "      <td>After all that, they complained to me about th...</td>\n",
       "      <td>[SERVICE#GENERAL]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        text                          categories\n",
       "id                                                                                              \n",
       "1004293:0  Judging from previous posts this used to be a ...                [RESTAURANT#GENERAL]\n",
       "1004293:1  We, there were four of us, arrived at noon - t...                   [SERVICE#GENERAL]\n",
       "1004293:2  They never brought us complimentary noodles, i...                   [SERVICE#GENERAL]\n",
       "1004293:3  The food was lousy - too sweet or too salty an...  [FOOD#QUALITY, FOOD#STYLE_OPTIONS]\n",
       "1004293:4  After all that, they complained to me about th...                   [SERVICE#GENERAL]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 11
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7w49ZkJIhtK",
    "colab_type": "text"
   },
   "source": [
    "    \"\"\"Load and display semeval16 testing dataset\n",
    "       Load - Load dataset using load_dataset method ( Reads formatted XML file from the provided path )\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OBTVOw9mnZj0",
    "colab_type": "code",
    "outputId": "ef7963a7-2af1-4480-baeb-775638af8da2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    }
   },
   "source": [
    "test_ds_path = DATA_PATHS['asba.semeval16.raw.test.gold']\n",
    "\n",
    "df_test = load_dataset(test_ds_path)\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    'text': df_test.groupby('id')['text'].first(),\n",
    "    'categories': df_test.groupby('id')['category'].apply(list),\n",
    "})\n",
    "\n",
    "df_test.head()"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en_BlueRibbonSushi_478218171:0</th>\n",
       "      <td>Yum!</td>\n",
       "      <td>[FOOD#QUALITY]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en_BlueRibbonSushi_478218171:1</th>\n",
       "      <td>Serves really good sushi.</td>\n",
       "      <td>[FOOD#QUALITY]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en_BlueRibbonSushi_478218171:2</th>\n",
       "      <td>Not the biggest portions but adequate.</td>\n",
       "      <td>[FOOD#STYLE_OPTIONS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en_BlueRibbonSushi_478218171:3</th>\n",
       "      <td>Green Tea creme brulee is a must!</td>\n",
       "      <td>[FOOD#QUALITY]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en_BlueRibbonSushi_478218171:4</th>\n",
       "      <td>Don't leave the restaurant without it.</td>\n",
       "      <td>[FOOD#QUALITY]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text            categories\n",
       "id                                                                                          \n",
       "en_BlueRibbonSushi_478218171:0                                    Yum!        [FOOD#QUALITY]\n",
       "en_BlueRibbonSushi_478218171:1               Serves really good sushi.        [FOOD#QUALITY]\n",
       "en_BlueRibbonSushi_478218171:2  Not the biggest portions but adequate.  [FOOD#STYLE_OPTIONS]\n",
       "en_BlueRibbonSushi_478218171:3       Green Tea creme brulee is a must!        [FOOD#QUALITY]\n",
       "en_BlueRibbonSushi_478218171:4  Don't leave the restaurant without it.        [FOOD#QUALITY]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 12
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4o7T18RXIFr2",
    "colab_type": "text"
   },
   "source": [
    "    \"\"\"Load and display Foursquare testing dataset\n",
    "       Load - Load dataset using load_dataset method ( Reads formatted XML file from the provided path )\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VeigSKIDxRuA",
    "colab_type": "code",
    "outputId": "2a1d8680-71a4-4fb3-e7c7-cb3c1460d0a2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    }
   },
   "source": [
    "test_fs_ds_path = DATA_PATHS['asba.foursquare.raw.test.gold']\n",
    "\n",
    "df_test_fs = load_dataset(test_fs_ds_path)\n",
    "\n",
    "df_test_fs = pd.DataFrame({\n",
    "    'text': df_test_fs.groupby('id')['text'].first(),\n",
    "    'categories': df_test_fs.groupby('id')['category'].apply(list),\n",
    "})\n",
    "\n",
    "df_test_fs.head()"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0:0</th>\n",
       "      <td>2 words -filter coffee :-)</td>\n",
       "      <td>[DRINKS#QUALITY]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101:0</th>\n",
       "      <td>Absolute favourite hotpot in town!</td>\n",
       "      <td>[RESTAURANT#GENERAL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102:0</th>\n",
       "      <td>Great great food, not too pricey, good service...</td>\n",
       "      <td>[FOOD#QUALITY, SERVICE#GENERAL, LOCATION#GENER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102:1</th>\n",
       "      <td>Totally recommended.</td>\n",
       "      <td>[RESTAURANT#GENERAL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103:0</th>\n",
       "      <td>Go to sandwich bar for lunch and split it with...</td>\n",
       "      <td>[FOOD#STYLE_OPTIONS, FOOD#PRICES]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text                                         categories\n",
       "id                                                                                                         \n",
       "0:0                           2 words -filter coffee :-)                                   [DRINKS#QUALITY]\n",
       "101:0                 Absolute favourite hotpot in town!                               [RESTAURANT#GENERAL]\n",
       "102:0  Great great food, not too pricey, good service...  [FOOD#QUALITY, SERVICE#GENERAL, LOCATION#GENER...\n",
       "102:1                               Totally recommended.                               [RESTAURANT#GENERAL]\n",
       "103:0  Go to sandwich bar for lunch and split it with...                  [FOOD#STYLE_OPTIONS, FOOD#PRICES]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 13
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-zVRBhU4-bY",
    "colab_type": "text"
   },
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noLJn5hS5Bea",
    "colab_type": "text"
   },
   "source": [
    "Preprocess Targets\n",
    "\n",
    "    \"\"\"As we have multiple aspects we have used MultiLabelBinarizer to create y_train, y_test and y_test_fs\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "et5IBOEQcOg0",
    "colab_type": "code",
    "outputId": "2c046a26-27a2-467e-82cc-3edb447928f6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "y_train = mlb.fit_transform(df_train.categories)\n",
    "\n",
    "y_test = mlb.transform(df_test.categories)\n",
    "\n",
    "y_test_fs = mlb.transform(df_test_fs.categories)\n",
    "\n",
    "y_train.shape, y_test.shape, y_test_fs.shape"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((1708, 12), (587, 12), (849, 12))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 14
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCp3bOF75ICN",
    "colab_type": "text"
   },
   "source": [
    "### Preprocess Features\n",
    "\n",
    "\"\"\" Preprocess features - Train and test data , pos are passed through spacy_doc pipeline to generate the list of tokens, which are later passed through keras tokenizer to convert them to integers. Finally they are padded to maxlenght . By performing above steps we have our x_train, x_test and x_test_fs input features \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LyEbXawNeSHG",
    "colab_type": "code",
    "outputId": "d3905ae4-71b8-4924-bb0c-01b9839d4667",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "maxlen = 50\n",
    "\n",
    "train_tokens = []\n",
    "train_pos_tags = []\n",
    "\n",
    "for sent in df_train.text:\n",
    "  spacy_doc = nlp(sent)\n",
    "  train_tokens.append([t.text for t in spacy_doc])\n",
    "  train_pos_tags.append([t.pos_ for t in spacy_doc])\n",
    "\n",
    "test_tokens = []\n",
    "test_pos_tags = []\n",
    "\n",
    "for sent in df_test.text:\n",
    "  spacy_doc = nlp(sent)\n",
    "  test_tokens.append([t.text for t in spacy_doc])\n",
    "  test_pos_tags.append([t.pos_ for t in spacy_doc])\n",
    "\n",
    "test_fs_tokens = []\n",
    "test_fs_pos_tags = []\n",
    "\n",
    "for sent in df_test_fs.text:\n",
    "  spacy_doc = nlp(sent)\n",
    "  test_fs_tokens.append([t.text for t in spacy_doc])\n",
    "  test_fs_pos_tags.append([t.pos_ for t in spacy_doc])\n",
    "\n",
    "#Convert words to integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_tokens + test_tokens + test_fs_tokens)\n",
    "x_train = tokenizer.texts_to_sequences(train_tokens)\n",
    "x_test = tokenizer.texts_to_sequences(test_tokens)\n",
    "x_test_fs = tokenizer.texts_to_sequences(test_fs_tokens)\n",
    "\n",
    "#Convert POS to integers\n",
    "pos_tokenizer = Tokenizer()\n",
    "pos_tokenizer.fit_on_texts(train_pos_tags + test_pos_tags + test_fs_pos_tags)\n",
    "x_pos_train = pos_tokenizer.texts_to_sequences(train_pos_tags)\n",
    "x_pos_test = pos_tokenizer.texts_to_sequences(test_pos_tags)\n",
    "x_pos_test_fs = pos_tokenizer.texts_to_sequences(test_fs_pos_tags)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_pos_train = pad_sequences(x_pos_train, maxlen=maxlen)\n",
    "\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "x_pos_test = pad_sequences(x_pos_test, maxlen=maxlen)\n",
    "\n",
    "x_test_fs = pad_sequences(x_test_fs, maxlen=maxlen)\n",
    "x_pos_test_fs = pad_sequences(x_pos_test_fs, maxlen=maxlen)\n",
    "\n",
    "x_train.shape, x_test.shape, x_test_fs.shape"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((1708, 50), (587, 50), (849, 50))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 15
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2sJLQ5c5YJ9",
    "colab_type": "text"
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hswRuNvxNWet",
    "colab_type": "text"
   },
   "source": [
    "\"\"\" create word embeddings and embedding_index based on word2vec \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C1GMwn81ly-F",
    "colab_type": "code",
    "outputId": "4b895220-3882-4414-cf96-94505e4144b3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    }
   },
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "seq_length = maxlen\n",
    "embedding_dim = word2vec.syn0.shape[1]\n",
    "vocab_size = len(word_index) + 1\n",
    "\n",
    "embeddings_index = word2vec\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "  if word in embeddings_index:\n",
    "    embedding_vector = embeddings_index[word]\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "  else:\n",
    "      pass # for words not in embedding index"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  after removing the cwd from sys.path.\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "w3EJJM-HDLMR",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#Extract vocabulary size and word embedding dimension\n",
    "max_tokens, dimension = len(pos_tokenizer.word_index) + 1, 300"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQ1Xn-caOJtG",
    "colab_type": "text"
   },
   "source": [
    "\"\"\" Metrics on_train_begin and on_epoch_end are callback functions used to print intermediate results during training\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NPmZz_6-ZD8Z",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class Metrics(Callback):\n",
    "  def __init__(self, model, validation_data):\n",
    "    self.model_ = model\n",
    "    self.validation_data = validation_data\n",
    "\n",
    "  def on_train_begin(self, logs={}):\n",
    "    print('epoch, precision_micro, recall_micro, f1_micro')\n",
    "  \n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    val_predict = (np.asarray(self.model_.predict(self.validation_data[0]))).round()\n",
    "    val_targ = self.validation_data[1]\n",
    "    _val_precision = precision_score(val_targ, val_predict, average='micro')\n",
    "    _val_recall = recall_score(val_targ, val_predict, average='micro')\n",
    "    _val_f1 = f1_score(val_targ, val_predict, average='micro')\n",
    "    print('%d, %f, %f, %f' % (epoch + 1, _val_precision, _val_recall, _val_f1))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Z0alYU9OeoY",
    "colab_type": "text"
   },
   "source": [
    "\"\"\" Attention layer is created , reference to the source code usage has been provided \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UgknNmzyR2O1",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# from https://github.com/philipperemy/keras-attention-mechanism/blob/master/attention/attention.py\n",
    "def attention_3d_block(hidden_states):\n",
    "    \"\"\"\n",
    "    Many-to-one attention mechanism for Keras.\n",
    "    @param hidden_states: 3D tensor with shape (batch_size, time_steps, input_dim).\n",
    "    @return: 2D tensor with shape (batch_size, 128)\n",
    "    @author: felixhao28.\n",
    "    \"\"\"\n",
    "    hidden_size = int(hidden_states.shape[2])\n",
    "    # Inside dense layer\n",
    "    #              hidden_states            dot               W            =>           score_first_part\n",
    "    # (batch_size, time_steps, hidden_size) dot (hidden_size, hidden_size) => (batch_size, time_steps, hidden_size)\n",
    "    # W is the trainable weight matrix of attention Luong's multiplicative style score\n",
    "    score_first_part = Dense(hidden_size, use_bias=False, name='attention_score_vec')(hidden_states)\n",
    "    #            score_first_part           dot        last_hidden_state     => attention_weights\n",
    "    # (batch_size, time_steps, hidden_size) dot   (batch_size, hidden_size)  => (batch_size, time_steps)\n",
    "    h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name='last_hidden_state')(hidden_states)\n",
    "    score = dot([score_first_part, h_t], [2, 1], name='attention_score')\n",
    "    attention_weights = Activation('softmax', name='attention_weight')(score)\n",
    "    # (batch_size, time_steps, hidden_size) dot (batch_size, time_steps) => (batch_size, hidden_size)\n",
    "    context_vector = dot([hidden_states, attention_weights], [1, 1], name='context_vector')\n",
    "    pre_activation = concatenate([context_vector, h_t], name='attention_output')\n",
    "    # attention_vector = Dense(128, use_bias=False, activation='tanh', name='attention_vector')(pre_activation)\n",
    "    return pre_activation"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVVH89sHPvZm",
    "colab_type": "text"
   },
   "source": [
    "\"\"\"create_model - Create different models such as 'LSTM', 'self-attention', 'pos-attention', 'bidirectional. For all the models we have one layer of 128 hidden units, and a fully connected output layer using sigmoid as activation function. We have used Adam optimizer, and cross-entropy for the loss function with learning rate 0.001 for all the models.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "URTgKkfOfAKw",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "lstm_dropout = 0.8\n",
    "lstm_out = 128\n",
    "use_model =  ['LSTM', 'self-attention', 'pos-attention', 'bidirectional'][3]\n",
    "loss = 'binary_crossentropy'\n",
    "def create_model():\n",
    "  embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=seq_length)\n",
    "\n",
    "  optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "  # Word Embedding\n",
    "  word_input_layer = Input(shape=(seq_length,))\n",
    "  word_embedding_output = embedding_layer(word_input_layer)\n",
    "\n",
    "  # POS Embedding\n",
    "  if use_model == 'pos-attention':\n",
    "    pos_embedding_layer = Embedding(max_tokens, dimension, input_length=seq_length)\n",
    "    pos_input_layer = Input(shape=(seq_length,), name='pos_seq')\n",
    "    pos_embedding_output = pos_embedding_layer(pos_input_layer)\n",
    "\n",
    "  # word_pos_attention_seq = Attention()([word_embedding_output, pos_embedding_output])\n",
    "\n",
    "  if use_model == 'self-attention':\n",
    "    lstm_output = LSTM(units=lstm_out, dropout=lstm_dropout, return_sequences=True)(word_embedding_output)\n",
    "    lstm_output = attention_3d_block(lstm_output)\n",
    "  elif use_model == 'pos-attention':\n",
    "    pass\n",
    "  elif use_model == 'bidirectional':\n",
    "    lstm_output = Bidirectional(LSTM(units=lstm_out, dropout=lstm_dropout))(word_embedding_output)\n",
    "  else:\n",
    "    lstm_output = LSTM(units=lstm_out, dropout=lstm_dropout)(word_embedding_output)\n",
    "\n",
    "  output_layer = Dense(12, activation='sigmoid')(lstm_output)\n",
    "\n",
    "  model = Model(inputs=[word_input_layer], outputs=[output_layer])\n",
    "  model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "  return model"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXMrxPgo3Xq_",
    "colab_type": "text"
   },
   "source": [
    "## Cross Validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50P7NqM0UBsC",
    "colab_type": "text"
   },
   "source": [
    "\"\"\"Perform 5-fold cross validation on the training dataset and use Metrics to derive intermediate evaluation results and finally we are using these results  we perform error analysis \"\"\"\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FCsNP8Dth4fB",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "if False: # Done\n",
    "    # Five fold Cross-Validation \n",
    "    y_preds = np.zeros(y_train.shape)\n",
    "    kf = KFold(n_splits=5)\n",
    "    for i, (train_index, valid_index) in enumerate(kf.split(x_train)):\n",
    "        model, metrics = None, None\n",
    "        print('Epoch: %d' % i)\n",
    "        x_train_1, x_valid, y_train_1, y_valid = x_train[train_index], x_train[valid_index], y_train[train_index], y_train[valid_index]\n",
    "        model = create_model()\n",
    "        metrics = Metrics(model, validation_data=(x_valid, y_valid))\n",
    "        model.fit(x_train_1, y_train_1, epochs=num_epochs, verbose=0, batch_size=batch_size, callbacks=[metrics])\n",
    "        # predict the results\n",
    "        score, acc = model.evaluate(x_valid, y_valid, verbose=2, batch_size=batch_size)\n",
    "        y_pred = model.predict(x_valid)\n",
    "        y_preds[valid_index] = y_pred\n",
    "        y_pred_bool = y_pred > threshold\n",
    "        # ROC AUC curve\n",
    "        roc_auc = roc_auc_score(y_valid, y_pred)\n",
    "        f1_micro = f1_score(y_valid, y_pred_bool, average='micro')\n",
    "        print(\"roc_auc, {}\".format(roc_auc))\n",
    "        report = classification_report(y_valid, y_pred_bool, target_names=list(mlb.classes_), output_dict=True)\n",
    "        df = pd.DataFrame(report).transpose()\n",
    "        print(df.to_csv())\n",
    "    df_train['predictions'] = [list(x) for x in mlb.inverse_transform(y_preds > 0.5)]\n",
    "    df_train.to_excel('.project/output/preds/ac_dl.xlsx')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0QKCyTW4bUu",
    "colab_type": "text"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kSs0h88ph0zb",
    "colab_type": "code",
    "outputId": "d2d579fb-a446-4043-94cc-54481c04c602",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    }
   },
   "source": [
    "#fit model\n",
    "batch_size = 32\n",
    "num_epochs = 15\n",
    "\n",
    "model = create_model()\n",
    "print(model.summary())\n",
    "\n",
    "x_train_1, x_valid, y_train_1, y_valid = train_test_split(x_train, y_train, test_size= 0.2, random_state = 24)\n",
    "metrics = Metrics(model, validation_data=(x_valid, y_valid))\n",
    "model.fit(x_train_1, y_train_1, epochs=num_epochs, batch_size=batch_size, validation_data=(x_valid, y_valid))"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 50, 300)           1341900   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               439296    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 12)                3084      \n",
      "=================================================================\n",
      "Total params: 1,784,280\n",
      "Trainable params: 1,784,280\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 0.3581 - accuracy: 0.2943 - val_loss: 0.2857 - val_accuracy: 0.2719\n",
      "Epoch 2/15\n",
      "43/43 [==============================] - 1s 21ms/step - loss: 0.2772 - accuracy: 0.3755 - val_loss: 0.2804 - val_accuracy: 0.2953\n",
      "Epoch 3/15\n",
      "43/43 [==============================] - 1s 21ms/step - loss: 0.2455 - accuracy: 0.4912 - val_loss: 0.2341 - val_accuracy: 0.5175\n",
      "Epoch 4/15\n",
      "43/43 [==============================] - 1s 21ms/step - loss: 0.2204 - accuracy: 0.5608 - val_loss: 0.2192 - val_accuracy: 0.5526\n",
      "Epoch 5/15\n",
      "43/43 [==============================] - 1s 21ms/step - loss: 0.2030 - accuracy: 0.5835 - val_loss: 0.2051 - val_accuracy: 0.6257\n",
      "Epoch 6/15\n",
      "43/43 [==============================] - 1s 21ms/step - loss: 0.1912 - accuracy: 0.6223 - val_loss: 0.1897 - val_accuracy: 0.6170\n",
      "Epoch 7/15\n",
      "43/43 [==============================] - 1s 21ms/step - loss: 0.1787 - accuracy: 0.6442 - val_loss: 0.1911 - val_accuracy: 0.6345\n",
      "Epoch 8/15\n",
      "43/43 [==============================] - 1s 21ms/step - loss: 0.1660 - accuracy: 0.6545 - val_loss: 0.1723 - val_accuracy: 0.6725\n",
      "Epoch 9/15\n",
      "43/43 [==============================] - 1s 21ms/step - loss: 0.1525 - accuracy: 0.6925 - val_loss: 0.1705 - val_accuracy: 0.6637\n",
      "Epoch 10/15\n",
      "43/43 [==============================] - 1s 22ms/step - loss: 0.1407 - accuracy: 0.7160 - val_loss: 0.1659 - val_accuracy: 0.6667\n",
      "Epoch 11/15\n",
      "43/43 [==============================] - 1s 21ms/step - loss: 0.1355 - accuracy: 0.7160 - val_loss: 0.1638 - val_accuracy: 0.6901\n",
      "Epoch 12/15\n",
      "43/43 [==============================] - 1s 20ms/step - loss: 0.1225 - accuracy: 0.7269 - val_loss: 0.1626 - val_accuracy: 0.6696\n",
      "Epoch 13/15\n",
      "43/43 [==============================] - 1s 20ms/step - loss: 0.1189 - accuracy: 0.7430 - val_loss: 0.1593 - val_accuracy: 0.6637\n",
      "Epoch 14/15\n",
      "43/43 [==============================] - 1s 20ms/step - loss: 0.1095 - accuracy: 0.7467 - val_loss: 0.1536 - val_accuracy: 0.6696\n",
      "Epoch 15/15\n",
      "43/43 [==============================] - 1s 21ms/step - loss: 0.0992 - accuracy: 0.7657 - val_loss: 0.1580 - val_accuracy: 0.6637\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa3e025c0f0>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 21
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PckDg1X4VOJ2",
    "colab_type": "text"
   },
   "source": [
    "\"\"\" Fit model on the complete training data set \"\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8u_hf-qoMVkH",
    "colab_type": "code",
    "outputId": "4ad05316-b141-4638-fa85-6f6ab4744062",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    }
   },
   "source": [
    "#fit model\n",
    "batch_size = 32\n",
    "num_epochs = 15\n",
    "\n",
    "model = create_model()\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(x_train, y_train, epochs=num_epochs, verbose=0, batch_size=batch_size)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 50, 300)           1341900   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               439296    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                3084      \n",
      "=================================================================\n",
      "Total params: 1,784,280\n",
      "Trainable params: 1,784,280\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa3b2733d30>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 22
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQ7wAvqX3-vE",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# This is formatted as code\n",
    "```\n",
    "\n",
    "## Dev Set Validate"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6URxaWZ0iUKO",
    "colab_type": "code",
    "outputId": "221df675-5ea8-480a-d3bf-b5b0d8646642",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    }
   },
   "source": [
    "threshold = 0.5\n",
    "\n",
    "# predict the results\n",
    "score, acc = model.evaluate(x_valid, y_valid, verbose=0, batch_size=batch_size)\n",
    "y_pred = model.predict(x_valid)\n",
    "\n",
    "y_pred_bool = y_pred > threshold\n",
    "\n",
    "# ROC AUC curve\n",
    "\n",
    "roc_auc = roc_auc_score(y_valid, y_pred)\n",
    "f1_micro = f1_score(y_valid, y_pred_bool, average='micro')\n",
    "\n",
    "print(\"roc_auc, {}\".format(roc_auc))\n",
    "\n",
    "report = classification_report(y_valid, y_pred_bool, target_names=list(mlb.classes_), output_dict=True)\n",
    "\n",
    "df = pd.DataFrame(report).transpose()\n",
    "\n",
    "print(df.to_csv())"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "roc_auc, 0.9697943753287932\n",
      ",precision,recall,f1-score,support\n",
      "AMBIENCE#GENERAL,0.9787234042553191,0.9583333333333334,0.968421052631579,48.0\n",
      "DRINKS#PRICES,1.0,0.3333333333333333,0.5,3.0\n",
      "DRINKS#QUALITY,1.0,0.2727272727272727,0.42857142857142855,11.0\n",
      "DRINKS#STYLE_OPTIONS,0.0,0.0,0.0,10.0\n",
      "FOOD#PRICES,0.9090909090909091,0.625,0.7407407407407406,16.0\n",
      "FOOD#QUALITY,0.9558823529411765,0.9027777777777778,0.9285714285714286,144.0\n",
      "FOOD#STYLE_OPTIONS,0.8,0.6,0.6857142857142857,20.0\n",
      "LOCATION#GENERAL,0.0,0.0,0.0,2.0\n",
      "RESTAURANT#GENERAL,0.922077922077922,0.9102564102564102,0.9161290322580644,78.0\n",
      "RESTAURANT#MISCELLANEOUS,1.0,0.36363636363636365,0.5333333333333333,22.0\n",
      "RESTAURANT#PRICES,0.7333333333333333,0.7333333333333333,0.7333333333333333,15.0\n",
      "SERVICE#GENERAL,0.9875,0.9634146341463414,0.9753086419753086,82.0\n",
      "micro avg,0.9440203562340967,0.8226164079822617,0.8791469194312796,451.0\n",
      "macro avg,0.7738839934748883,0.555234371545347,0.6175102730941252,451.0\n",
      "weighted avg,0.9203288574175548,0.8226164079822617,0.8561987273003602,451.0\n",
      "samples avg,0.9035087719298246,0.8564814814814814,0.8693191311612363,451.0\n",
      "\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObWwWc010ND1",
    "colab_type": "text"
   },
   "source": [
    "## Evaluate on the test data \n",
    "\n",
    "- Do not use this for comparing your models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OpLQtghy2epS",
    "colab_type": "code",
    "outputId": "503b2ec7-ac38-4461-a6f0-c13f24402637",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    }
   },
   "source": [
    "print(model.summary())"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 50, 300)           1341900   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               439296    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                3084      \n",
      "=================================================================\n",
      "Total params: 1,784,280\n",
      "Trainable params: 1,784,280\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxE_pIZ13w-O",
    "colab_type": "text"
   },
   "source": [
    "### SemEval-16 Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaQUdMdVWbgz",
    "colab_type": "text"
   },
   "source": [
    "\"\"\" Model predict on SemEval-16 test dataset\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nD_vUEdBVqx3",
    "colab_type": "code",
    "outputId": "7661c679-8ecb-4eae-e834-d6f538bd5764",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    }
   },
   "source": [
    "threshold = 0.5\n",
    "\n",
    "# predict the results\n",
    "score, acc = model.evaluate(x_test, y_test, verbose=0, batch_size=batch_size)\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_bool = y_pred > threshold\n",
    "# ROC AUC curve\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "f1_micro = f1_score(y_test, y_pred_bool, average='micro')\n",
    "\n",
    "print(\"roc_auc, {}\".format(roc_auc))\n",
    "\n",
    "report = classification_report(y_test, y_pred_bool, target_names=list(mlb.classes_), output_dict=True)\n",
    "\n",
    "df = pd.DataFrame(report).transpose()\n",
    "\n",
    "print(df.to_csv())"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "roc_auc, 0.9196456602764628\n",
      ",precision,recall,f1-score,support\n",
      "AMBIENCE#GENERAL,0.6470588235294118,0.7719298245614035,0.704,57.0\n",
      "DRINKS#PRICES,0.0,0.0,0.0,3.0\n",
      "DRINKS#QUALITY,1.0,0.047619047619047616,0.0909090909090909,21.0\n",
      "DRINKS#STYLE_OPTIONS,0.0,0.0,0.0,12.0\n",
      "FOOD#PRICES,0.6153846153846154,0.36363636363636365,0.4571428571428572,22.0\n",
      "FOOD#QUALITY,0.8678414096916299,0.8716814159292036,0.869757174392936,226.0\n",
      "FOOD#STYLE_OPTIONS,0.4482758620689655,0.2708333333333333,0.33766233766233766,48.0\n",
      "LOCATION#GENERAL,0.0,0.0,0.0,13.0\n",
      "RESTAURANT#GENERAL,0.824,0.7253521126760564,0.7715355805243446,142.0\n",
      "RESTAURANT#MISCELLANEOUS,0.0,0.0,0.0,33.0\n",
      "RESTAURANT#PRICES,0.6428571428571429,0.42857142857142855,0.5142857142857143,21.0\n",
      "SERVICE#GENERAL,0.9044117647058824,0.8482758620689655,0.8754448398576512,145.0\n",
      "micro avg,0.8084415584415584,0.6702557200538358,0.7328918322295805,743.0\n",
      "macro avg,0.49581913485313733,0.36065828236631686,0.3850614662312443,743.0\n",
      "weighted avg,0.7412085065028394,0.6702557200538358,0.6893199214006002,743.0\n",
      "samples avg,0.7404883588869959,0.7007382169222034,0.7088829398880506,743.0\n",
      "\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zitNOxvW3L0x",
    "colab_type": "text"
   },
   "source": [
    " Foursquare Test Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3lQCnTgWk2M",
    "colab_type": "text"
   },
   "source": [
    "\"\"\" Model predict on  test Foursqure dataset\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nxSMU-VVP4On",
    "colab_type": "code",
    "outputId": "063da5fa-1194-413e-8ed8-d6d77fb19257",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    }
   },
   "source": [
    "threshold = 0.5\n",
    "# predict the results\n",
    "y_pred_fs = model.predict(x_test_fs)\n",
    "y_pred_fs_bool = y_pred_fs > threshold\n",
    "# ROC AUC curve\n",
    "roc_auc = roc_auc_score(y_test_fs, y_pred_fs)\n",
    "print(\"roc_auc, {}\".format(roc_auc))\n",
    "# Classification Report\n",
    "report = classification_report(y_test_fs, y_pred_fs_bool, target_names=list(mlb.classes_), output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "print(df.to_csv())"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "roc_auc, 0.8726199915650126\n",
      ",precision,recall,f1-score,support\n",
      "AMBIENCE#GENERAL,0.7090909090909091,0.582089552238806,0.6393442622950819,67.0\n",
      "DRINKS#PRICES,1.0,0.16666666666666666,0.2857142857142857,6.0\n",
      "DRINKS#QUALITY,1.0,0.01282051282051282,0.02531645569620253,78.0\n",
      "DRINKS#STYLE_OPTIONS,0.0,0.0,0.0,11.0\n",
      "FOOD#PRICES,0.2857142857142857,0.09523809523809523,0.14285714285714285,21.0\n",
      "FOOD#QUALITY,0.8393285371702638,0.7954545454545454,0.8168028004667445,440.0\n",
      "FOOD#STYLE_OPTIONS,0.43902439024390244,0.3333333333333333,0.37894736842105264,54.0\n",
      "LOCATION#GENERAL,0.0,0.0,0.0,14.0\n",
      "RESTAURANT#GENERAL,0.4581005586592179,0.5694444444444444,0.5077399380804953,144.0\n",
      "RESTAURANT#MISCELLANEOUS,0.0,0.0,0.0,23.0\n",
      "RESTAURANT#PRICES,0.5882352941176471,0.3125,0.40816326530612246,32.0\n",
      "SERVICE#GENERAL,0.9029126213592233,0.6458333333333334,0.7530364372469636,144.0\n",
      "micro avg,0.7250608272506083,0.5764023210831721,0.6422413793103449,1034.0\n",
      "macro avg,0.5185338830296207,0.29278170696081146,0.3298268296736743,1034.0\n",
      "weighted avg,0.7208224290827833,0.5764023210831721,0.6034762548060101,1034.0\n",
      "samples avg,0.635257165292501,0.6002748331370239,0.6078047002075272,1034.0\n",
      "\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKoOe3u7Ch4R",
    "colab_type": "text"
   },
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaBY4S_AWpWy",
    "colab_type": "text"
   },
   "source": [
    "\"\"\" Save the required model, tokenizer and mlb for demo purposes \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KS3shxdc3CU2",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import pickle\n",
    "\n",
    "model.save('/content/project/output/models/dl_model_ac.h5')\n",
    "\n",
    "# saving tokenizer\n",
    "with open('/content/project/output/models/tokenizer_ac.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# saving mlb\n",
    "with open('/content/project/output/models/mlb_ac.pickle', 'wb') as handle:\n",
    "    pickle.dump(mlb, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i8EDXxyITVhT",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    ""
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}