{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%    \n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "# use logging to save the results\n",
    "# logging.basicConfig(filename='names_entity_recognition_RNN.log', level=logging.INFO)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "embedding_vector = models.KeyedVectors.load_word2vec_format(\n",
    "    './Data/conll2003/GoogleNews-vectors-negative300.bin', binary=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_files(path):\n",
    "    \"\"\"\n",
    "    read_files - helps to navigate through the files in the folder structure, read the files and convert them to\n",
    "    dataframe which consists of tweet\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, sep=\" \")\n",
    "    df = df.astype(str)\n",
    "    df.drop(df.columns[[1, 2]], axis=1, inplace=True)\n",
    "    df = df.rename(columns={\"-DOCSTART-\": \"words\", \"O\": \"entity\"})\n",
    "    # print(df.head())\n",
    "    df[\"words\"] = [word.lower() if not word.isupper() else word for word in df[\"words\"]]\n",
    "    print(df.head(30))\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_sentences(path):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    sentence = \"\"\n",
    "    label = \"\"\n",
    "    for line in open(path, \"r\").readlines():\n",
    "        if len(line.strip()) == 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = \"\"\n",
    "            labels.append(label.replace(\"\\n\", \"\"))\n",
    "            label = \"\"\n",
    "        else:\n",
    "            sentence = sentence + (line.split(\" \", 1)[0]) + \" \"\n",
    "            label = label + (line.split(\" \")[3]) + \" \"\n",
    "\n",
    "    # max_length_sentence = len(max(sentences, key=len))\n",
    "    return sentences[1:], labels[1:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pad_tag(df, sentences, max_length_sentence):\n",
    "    start_index = 0\n",
    "    for sentence in sentences:\n",
    "        # print(df[start_index:start_index+len(sentence.split())])\n",
    "        df[start_index:start_index + len(sentence.split())].to_csv(r'./Data/conll2003/train2.txt',\n",
    "                                                                   header=None, index=None, sep=' ', mode='a')\n",
    "        file1 = open(\"./Data/conll2003/train2.txt\", \"a\")\n",
    "        pad_length = max_length_sentence - len(sentence.split())\n",
    "        file1.write(\"0 <pad> \\n\" * pad_length + ('\\n'))\n",
    "\n",
    "        file1.close()\n",
    "        start_index = start_index + len(sentence.split())\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    run - Execution of appropriate functions as per the required call\n",
    "    \"\"\"\n",
    "    df_train = read_files('./Data/conll2003/train.txt')\n",
    "    sentences, labels = get_sentences('./Data/conll2003/train.txt')\n",
    "\n",
    "    max_length_sentence = len(max(sentences, key=len))\n",
    "    pad_tag(df_train, sentences, max_length_sentence)\n",
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    main - runs all the modules via run function\n",
    "    \"\"\"\n",
    "    # logging.info('AIT_726 Named Entity Recognition')\n",
    "    # logging.info('Authors: Yasas, Prashanti , Ashwini')\n",
    "    run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}